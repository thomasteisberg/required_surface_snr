{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dask\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import corner\n",
    "import pickle\n",
    "import cloudpickle\n",
    "\n",
    "from normalization_utils import fit_combo_scaler, inverse_combo_scaler\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select dataset to work with here\n",
    "\n",
    "Uncomment one of these lines only to define the dataset we're working with.\n",
    "\n",
    "Key to BedMachine Masks:\n",
    "\n",
    "0. Ocean\n",
    "1. Ice-free land\n",
    "2. Grounded\n",
    "3. Floating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CReSIS Greenland Grounded\n",
    "# dataset_name, ice_sheet, bm_mask_whitelist, input_data_path = (\n",
    "#     \"cresis_gis_grounded\", \"greenland\", [2], \n",
    "#     \"../data_preprocessing/snr_data_cresis_gis_with_inputs.nc\"\n",
    "# )\n",
    "\n",
    "# # UTIG Antarctica Grounded\n",
    "# dataset_name, ice_sheet, bm_mask_whitelist, input_data_path = (\n",
    "#     \"utig_ais_grounded\", \"antarctica\", [2], \n",
    "#     \"../data_preprocessing/snr_data_utig_ais_with_inputs.nc\"\n",
    "# )\n",
    "\n",
    "# # CReSIS Antarctica Grounded\n",
    "# dataset_name, ice_sheet, bm_mask_whitelist, input_data_path = (\n",
    "#     \"cresis_ais_grounded\", \"antarctica\", [2], \n",
    "#     \"../data_preprocessing/snr_data_cresis_ais_with_inputs.nc\"\n",
    "# )\n",
    "\n",
    "# CReSIS Antarctica Floating\n",
    "dataset_name, ice_sheet, bm_mask_whitelist, input_data_path = (\n",
    "    \"cresis_ais_floating\", \"antarctica\", [3], \n",
    "    \"../data_preprocessing/snr_data_cresis_ais_with_inputs.nc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any missing values and subset to variables of interest\n",
    "input_df = xr.open_dataset(input_data_path).to_dataframe()\n",
    "\n",
    "# Keep only the variables of interest\n",
    "vars_of_interest = ['snr', 'thickness', 'speed', 't2m', 'surface', 'bm_mask']\n",
    "if 'picked_thickness' in input_df.columns:\n",
    "    vars_of_interest.append('picked_thickness')\n",
    "input_df = input_df[vars_of_interest]\n",
    "input_df = input_df.dropna()\n",
    "\n",
    "# Add log(speed)\n",
    "input_df['log_speed'] = np.log(input_df['speed'])\n",
    "input_df = input_df[~np.isinf(input_df['log_speed'])]\n",
    "\n",
    "# Filter by BedMachine mask\n",
    "input_df = input_df[input_df['bm_mask'].isin(bm_mask_whitelist)]\n",
    "\n",
    "# Filter out zero thickness picks if available\n",
    "if 'picked_thickness' in input_df.columns:\n",
    "    input_df = input_df[input_df['picked_thickness'] > 0]\n",
    "\n",
    "# UTIG SNR is inverted\n",
    "if 'utig' in dataset_name:\n",
    "    input_df['snr'] = -input_df['snr']\n",
    "\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_norm = ['snr', 'thickness', 't2m', 'surface']\n",
    "\n",
    "# Normalize inputs and outputs\n",
    "normalization_parameters = {}\n",
    "for var in vars_to_norm:\n",
    "    input_df[var + \"_norm\"], normalization_parameters[var] = fit_combo_scaler(input_df[var])\n",
    "\n",
    "# Plot histograms with/without normalization\n",
    "\n",
    "fig, axs = plt.subplots(len(vars_to_norm), 2, figsize=(6, 1.5 * len(vars_to_norm)))\n",
    "for i, var in enumerate(vars_to_norm):\n",
    "    axs[i, 0].hist(input_df[var], bins=20)\n",
    "    axs[i, 0].set_title(f\"{var} histogram\")\n",
    "    axs[i, 1].hist(input_df[var + \"_norm\"], bins=20)\n",
    "    axs[i, 1].set_title(f\"{var}_norm histogram\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_like(theta, thickness_obs, surf_temp_obs, surf_elev_obs, snr_sim):\n",
    "    beta_0, beta_thickness, beta_surf_temp, beta_surf_elev = theta\n",
    "    model = beta_0 * np.exp(beta_thickness*thickness_obs +\n",
    "                            beta_surf_temp*surf_temp_obs +\n",
    "                            beta_surf_elev*surf_elev_obs\n",
    "                           )\n",
    "    \n",
    "    ln_z = -0.5 * np.sum((snr_sim - model)**2 + np.log(2*np.pi))\n",
    "    \n",
    "    return ln_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = lambda *args: -ln_like(*args)\n",
    "initial = np.repeat(0.0,4) + 0.1 * np.random.randn(4)\n",
    "likelihood = scipy.optimize.minimize(nll, initial, \n",
    "                      args=(input_df['thickness_norm'], input_df['t2m_norm'],\n",
    "                            input_df['surface_norm'], input_df['snr_norm']), \n",
    "                      method='BFGS')\n",
    "beta_0_ml, beta_thickness_ml, beta_surf_temp_ml, beta_surf_elev_ml = likelihood.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pm.Model()\n",
    "\n",
    "with model:\n",
    "\n",
    "    # Define weakly informative Normal priors for Ridge regression\n",
    "    sigma = pm.HalfNormal(\"sigma\", 1) #recommended for StudentT\n",
    "    b0 = pm.Normal(\"intercept\", beta_0_ml, sigma=10)\n",
    "    b1 = pm.Normal(\"beta_thickness\", beta_thickness_ml, sigma=10)\n",
    "    b2 = pm.Normal(\"beta_surface_temp\", beta_surf_temp_ml, sigma=10)\n",
    "    b3 = pm.Normal(\"beta_surface_elev\", beta_surf_elev_ml, sigma=10)\n",
    "    # b_EX = pm.Normal(\"name\", 0, sigma=0.1) #heavily regularizing prior, might be useful for colinear situations ahead\n",
    "    \n",
    "    d1 = pm.Data(\"thickness_norm\", input_df['thickness_norm'])\n",
    "    d2 = pm.Data(\"t2m_norm\", input_df['t2m_norm'])\n",
    "    d3 = pm.Data(\"surface_norm\", input_df['surface_norm'])\n",
    "\n",
    "    # Define linear model\n",
    "    y_est = b0 + b1*d1 + b2*d2 + b3*d3\n",
    "\n",
    "    # Define prior for StudentT degrees of freedom\n",
    "    # Inverse Gamma is recommended\n",
    "    nu = pm.InverseGamma(\"nu\", alpha=1, beta=1)\n",
    "\n",
    "    # Define Student T likelihood, because of the presence of outliers\n",
    "    likelihood = pm.StudentT(\n",
    "        \"likelihood\", mu=y_est, sigma=sigma, nu=nu, observed=input_df['snr_norm']\n",
    "    )\n",
    "    \n",
    "    trace = pm.sample(1000, cores=3,\n",
    "                          target_accept=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corner plot\n",
    "fig = corner.corner(np.vstack((trace.posterior['intercept'][0],\n",
    "                               trace.posterior['beta_thickness'][0],\n",
    "                               trace.posterior['beta_surface_temp'][0],\n",
    "                               trace.posterior['beta_surface_elev'][0],\n",
    "                               trace.posterior['sigma'][0])).T, \n",
    "                    labels = ['intercept', 'thickness', \n",
    "                              'T$_{surf}$', 'Elevation$_{surf}$',\n",
    "                              '$\\sigma$'], color='#1f77b4',\n",
    "                    alpha=0.25, \n",
    "                    label_kwargs={\"fontsize\": 22, \"labelpad\": 5}, \n",
    "                    quantiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace, hdi_prob = 0.68, round_to=3).loc[['intercept', \n",
    "                                                    'beta_thickness', \n",
    "                                                    'beta_surface_temp',\n",
    "                                                    'beta_surface_elev', \n",
    "                                                    'sigma'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, \n",
    "              var_names=['intercept', 'beta_thickness', \n",
    "                         'beta_surface_temp', 'beta_surface_elev', \n",
    "                         'sigma'], \n",
    "              combined=False, compact=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'dataset_name': dataset_name,\n",
    "    'ice_sheet': ice_sheet,\n",
    "    'bm_mask_whitelist': bm_mask_whitelist,\n",
    "    'input_data_path': input_data_path,\n",
    "    'trace': trace,\n",
    "    'model': model,\n",
    "    'normalization_parameters': normalization_parameters\n",
    "}\n",
    "\n",
    "# Save the model\n",
    "with open(f\"outputs/{dataset_name}_model.pickle\", \"wb\") as f:\n",
    "    cloudpickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rssnr2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
